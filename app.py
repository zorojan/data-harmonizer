import streamlit as st
import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.cluster import DBSCAN
import os

st.title("Category Similarity & Merging")

# === –§–£–ù–ö–¶–ò–ò –î–õ–Ø –ò–ï–†–ê–†–•–ò–ß–ï–°–ö–û–ô –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò ===

def parse_hierarchical_categories(df, category_col, separator="/"):
    """
    –ü–∞—Ä—Å–∏—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ —É—Ä–æ–≤–Ω–∏
    –ù–∞–ø—Ä–∏–º–µ—Ä: "Bench Tools/Circular saws" ‚Üí Level1: "Bench Tools", Level2: "Circular saws"
    """
    df_expanded = df.copy()
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—é
    split_cats = df_expanded[category_col].astype(str).str.split(separator, expand=True)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É—Ä–æ–≤–Ω–µ–π
    max_levels = split_cats.shape[1]
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è
    for level in range(max_levels):
        level_name = f"category_level_{level + 1}"
        df_expanded[level_name] = split_cats[level].str.strip() if level < split_cats.shape[1] else None
    
    # –û—á–∏—â–∞–µ–º –ø—É—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    for level in range(1, max_levels + 1):
        level_name = f"category_level_{level}"
        if level_name in df_expanded.columns:
            df_expanded[level_name] = df_expanded[level_name].fillna('')
    
    return df_expanded, max_levels

def get_hierarchical_embeddings(categories, model, max_levels=3):
    """
    –°–æ–∑–¥–∞–µ—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Å –≤–µ—Å–∞–º–∏ –ø–æ —É—Ä–æ–≤–Ω—è–º
    """
    all_embeddings = []
    
    for category in categories:
        # –†–∞–∑–±–∏–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –Ω–∞ —É—Ä–æ–≤–Ω–∏
        levels = [level.strip() for level in str(category).split('/') if level.strip()]
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ä–æ–≤–Ω—è —Å –≤–µ—Å–∞–º–∏
        level_embeddings = []
        
        for i, level in enumerate(levels[:max_levels]):
            if level:
                # –í–µ—Å —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è —Å –≥–ª—É–±–∏–Ω–æ–π —É—Ä–æ–≤–Ω—è (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤–µ—Ä—Ö–Ω–∏–º —É—Ä–æ–≤–Ω—è–º)
                weight = 1.0 / (i + 1)  # 1.0, 0.5, 0.33, 0.25...
                
                level_embedding = model.encode([level])[0]
                level_embeddings.append(level_embedding * weight)
        
        if level_embeddings:
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π
            combined_embedding = np.mean(level_embeddings, axis=0)
        else:
            # Fallback –¥–ª—è –ø—É—Å—Ç—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
            combined_embedding = model.encode([str(category)])[0]
        
        all_embeddings.append(combined_embedding)
    
    return np.array(all_embeddings)

def hierarchical_clustering_by_levels(df, category_col, model, eps, min_samples, separator="/"):
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –ø–æ —É—Ä–æ–≤–Ω—è–º
    """
    # 1. –ü–∞—Ä—Å–∏–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
    df_expanded, max_levels = parse_hierarchical_categories(df, category_col, separator)
    
    results = {}
    
    # 2. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –∫–∞–∂–¥–æ–º—É —É—Ä–æ–≤–Ω—é (–Ω–∞—á–∏–Ω–∞—è —Å —Å–∞–º–æ–≥–æ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ)
    for level in range(max_levels, 0, -1):
        level_name = f"category_level_{level}"
        
        if level_name not in df_expanded.columns:
            continue
            
        # –ü–æ–ª—É—á–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —ç—Ç–æ–≥–æ —É—Ä–æ–≤–Ω—è
        level_categories = df_expanded[df_expanded[level_name] != ''][level_name].unique()
        
        if len(level_categories) < 2:
            continue
            
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —ç—Ç–æ–≥–æ —É—Ä–æ–≤–Ω—è
        level_embeddings = model.encode(level_categories)
        
        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        clustering = DBSCAN(eps=eps, min_samples=min_samples, metric="cosine").fit(level_embeddings)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        level_clusters = pd.DataFrame({
            "category": level_categories,
            "cluster": clustering.labels_,
            "level": level
        })
        
        results[f"level_{level}"] = level_clusters
        
        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
        clusters_count = len(set(clustering.labels_)) - (1 if -1 in clustering.labels_ else 0)
        st.write(f"**–£—Ä–æ–≤–µ–Ω—å {level} ({level_name}):** {len(level_categories)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π ‚Üí {clusters_count} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
    
    return results, df_expanded

def hierarchical_clustering(df, category_col, separator="/", strategy="weighted_combined", threshold=0.35):
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    
    Args:
        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏
        category_col: Series —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ 
        separator: —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —É—Ä–æ–≤–Ω–µ–π –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏—è—Ö
        strategy: —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        threshold: –ø–æ—Ä–æ–≥ –¥–ª—è DBSCAN
    
    Returns:
        DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    """
    import numpy as np
    from sentence_transformers import SentenceTransformer
    from sklearn.cluster import DBSCAN
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
    model = SentenceTransformer('all-MiniLM-L6-v2')
    
    # –ü–∞—Ä—Å–∏–Ω–≥ –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π
    df_expanded, max_levels = parse_hierarchical_categories(df, category_col, separator)
    
    if strategy == "detailed_first":
        # –°–Ω–∞—á–∞–ª–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑—É–µ–º —Å–∞–º—ã–µ –¥–µ—Ç–∞–ª—å–Ω—ã–µ, –ø–æ—Ç–æ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ
        final_clusters = {}
        cluster_id = 0
        
        for level in range(max_levels, 0, -1):  # –û—Ç —Å–∞–º–æ–≥–æ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∫ –æ–±—â–µ–º—É
            level_name = f"category_level_{level}"
            if level_name not in df_expanded.columns:
                continue
                
            # –ë–µ—Ä–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —ç—Ç–æ–≥–æ —É—Ä–æ–≤–Ω—è (–Ω–µ –ø—É—Å—Ç—ã–µ)
            level_data = df_expanded[df_expanded[level_name] != '']
            if len(level_data) == 0:
                continue
                
            categories = level_data[level_name].unique()
            embeddings = model.encode(categories.tolist())
            
            clustering = DBSCAN(eps=threshold, min_samples=1, metric="cosine").fit(embeddings)
            
            for i, cat in enumerate(categories):
                if clustering.labels_[i] != -1:
                    # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ –∏—Å—Ö–æ–¥–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è —ç—Ç–æ–≥–æ —É—Ä–æ–≤–Ω—è
                    original_cats = level_data[level_data[level_name] == cat][category_col].unique()
                    for orig_cat in original_cats:
                        if orig_cat not in final_clusters:
                            final_clusters[orig_cat] = cluster_id
                    cluster_id += 1
                    
    elif strategy == "parent_fallback":
        # –°–Ω–∞—á–∞–ª–∞ –ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –µ—Å–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ - —Ä–æ–¥–∏—Ç–µ–ª–∏
        final_clusters = {}
        cluster_id = 0
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∏—Å—Ö–æ–¥–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        for orig_cat in df_expanded[category_col].unique():
            if orig_cat in final_clusters:
                continue
                
            # –ü—Ä–æ–±—É–µ–º –æ—Ç —Å–∞–º–æ–≥–æ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è –∫ –æ–±—â–µ–º—É
            clustered = False
            for level in range(max_levels, 0, -1):
                level_name = f"category_level_{level}"
                if level_name not in df_expanded.columns:
                    continue
                    
                # –ü–æ–ª—É—á–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ —ç—Ç–æ–≥–æ —É—Ä–æ–≤–Ω—è –¥–ª—è –¥–∞–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                cat_data = df_expanded[df_expanded[category_col] == orig_cat]
                if len(cat_data) == 0:
                    continue
                    
                level_value = cat_data[level_name].iloc[0]
                if level_value == '':
                    continue
                
                # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –Ω–∞ —ç—Ç–æ–º —É—Ä–æ–≤–Ω–µ
                similar_cats = df_expanded[
                    (df_expanded[level_name] == level_value) &
                    (~df_expanded[category_col].isin(final_clusters.keys()))
                ]
                
                if len(similar_cats) > 1:  # –ï—Å—Ç—å –ø–æ—Ö–æ–∂–∏–µ
                    for _, row in similar_cats.iterrows():
                        final_clusters[row[category_col]] = cluster_id
                    cluster_id += 1
                    clustered = True
                    break
                    
            if not clustered:
                final_clusters[orig_cat] = -1  # ungrouped
                
    else:  # weighted_combined
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –≤—Å–µ–º —É—Ä–æ–≤–Ω—è–º
        categories = df[category_col].unique()
        hierarchical_embeddings = get_hierarchical_embeddings(categories, model)
        
        clustering = DBSCAN(eps=threshold, min_samples=1, metric="cosine").fit(hierarchical_embeddings)
        
        final_clusters = dict(zip(categories, clustering.labels_))
    
    # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∏–π DataFrame
    result_df = pd.DataFrame([
        {'category': cat, 'cluster': cluster_id}
        for cat, cluster_id in final_clusters.items()
    ])
    
    return result_df

# 1. Upload files or load demo data
st.markdown('---')
demo_files = [
    os.path.join("Upload", "source1.csv"),
    os.path.join("Upload", "source2.csv"),
    os.path.join("Upload", "source3.csv"),
    os.path.join("Upload", "source4.csv")
]

def robust_read_csv(file_or_path):
    """
    More robust CSV reading function that tries different encodings,
    delimiters, and handles bad lines to prevent ParserError.
    Now supports complex headers with quotes, brackets, and special characters.
    """
    # For file-like objects from upload, we need to be able to seek 
    is_file_like = hasattr(file_or_path, 'seek')

    # List of configurations to try - –¥–æ–±–∞–≤–ª–µ–Ω –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π
    configs = [
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Ç–æ—á–∫–∏ —Å –∑–∞–ø—è—Ç–æ–π (—á–∞—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –µ–≤—Ä–æ–ø–µ–π—Å–∫–∏—Ö CSV)
        {'encoding': 'utf-8', 'sep': ';', 'on_bad_lines': 'warn', 'quotechar': '"'},
        {'encoding': 'cp1251', 'sep': ';', 'on_bad_lines': 'warn', 'quotechar': '"'},
        {'encoding': 'utf-8-sig', 'sep': ';', 'on_bad_lines': 'warn', 'quotechar': '"'},  # BOM support
        {'encoding': 'windows-1252', 'sep': ';', 'on_bad_lines': 'warn', 'quotechar': '"'},
        
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∑–∞–ø—è—Ç—ã–µ
        {'encoding': 'utf-8', 'sep': ',', 'on_bad_lines': 'warn', 'quotechar': '"'},
        {'encoding': 'cp1251', 'sep': ',', 'on_bad_lines': 'warn', 'quotechar': '"'},
        {'encoding': 'utf-8-sig', 'sep': ',', 'on_bad_lines': 'warn', 'quotechar': '"'},
        
        # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—è
        {'encoding': 'utf-8', 'sep': None, 'engine': 'python', 'on_bad_lines': 'skip', 'quotechar': '"'},
        {'encoding': 'cp1251', 'sep': None, 'engine': 'python', 'on_bad_lines': 'skip', 'quotechar': '"'},
        
        # –ë–µ–∑ –∫–∞–≤—ã—á–µ–∫ (–µ—Å–ª–∏ —Ñ–∞–π–ª –∏–º–µ–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã —Å –∫–∞–≤—ã—á–∫–∞–º–∏)
        {'encoding': 'utf-8', 'sep': ';', 'on_bad_lines': 'warn', 'quoting': 3},  # QUOTE_NONE
        {'encoding': 'utf-8', 'sep': ',', 'on_bad_lines': 'warn', 'quoting': 3},
        
        # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é
        {'encoding': 'latin1', 'sep': ';', 'engine': 'python', 'on_bad_lines': 'skip'},
        {'encoding': 'latin1', 'sep': ',', 'engine': 'python', 'on_bad_lines': 'skip'}
    ]

    last_error = None
    for i, config in enumerate(configs):
        try:
            if is_file_like:
                file_or_path.seek(0)
            
            df = pd.read_csv(file_or_path, **config)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –∑–∞–≥—Ä—É–∑–∫–∏
            if len(df.columns) > 1 and len(df) > 0:
                # –û—á–∏—â–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –æ—Ç –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
                df.columns = [str(col).strip().strip('"').strip("'") for col in df.columns]
                
                # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                sep_name = config.get('sep', 'auto-detect')
                encoding_name = config.get('encoding', 'default')
                if i < 3:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–µ—Ä–≤—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
                    st.info(f"‚úÖ CSV –∑–∞–≥—Ä—É–∂–µ–Ω: —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å='{sep_name}', –∫–æ–¥–∏—Ä–æ–≤–∫–∞='{encoding_name}', –∫–æ–ª–æ–Ω–æ–∫={len(df.columns)}")
                
                return df
        except (UnicodeDecodeError, pd.errors.ParserError, ValueError, Exception) as e:
            last_error = e
            continue # Try next configuration

    # If all attempts fail, raise an error with details
    file_name = getattr(file_or_path, 'name', str(file_or_path))
    st.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å CSV —Ñ–∞–π–ª: {file_name}")
    st.error(f"–ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {last_error}")
    st.warning("üí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ:")
    st.warning("- –ü–µ—Ä–µ—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∞–π–ª –≤ UTF-8 –∫–æ–¥–∏—Ä–æ–≤–∫–µ")
    st.warning("- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å CSV —Ñ–æ—Ä–º–∞—Ç–∞")
    st.warning("- –£–±–µ–¥–∏—Ç—å—Å—è —á—Ç–æ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ –∏ –∫–∞–≤—ã—á–∫–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    return None


col1, col2 = st.columns(2)
with col1:
    uploaded_files = st.file_uploader("Upload CSV files", type=["csv"], accept_multiple_files=True, key="uploader")
with col2:
    use_demo = st.session_state.get('use_demo', False)
    if st.button("Load demo data from server"):
        st.session_state['use_demo'] = True
        use_demo = True

if st.session_state.get('use_demo', False):
    dfs = [robust_read_csv(f) for f in demo_files]
    # Filter out None results if a file failed to load
    dfs = [df for df in dfs if df is not None]
    if not dfs:
        st.stop() # Stop if no demo files could be loaded

    selected_dfs = []
    for i, df_demo in enumerate(dfs):
        df_demo['source_file'] = os.path.basename(demo_files[i])
        
        with st.expander(f"üìÅ Demo file: {os.path.basename(demo_files[i])}", expanded=False):
            st.dataframe(df_demo)        
            columns = list(df_demo.columns)
            # Try to auto-detect columns  
            def find_col(targets):
                for t in targets:
                    for c in columns:
                        if c.strip().lower() == t:
                            return c
                return ''
            # Also match 'Product Name' to 'product_name' for standardization
            product_guess = find_col(['product_name', 'product', 'name', 'product name'])
            category_guess = find_col(['category', 'cat'])
            sku_guess = find_col(['sku', '–∫–æ–¥', 'id'])
            col1, col2, col3 = st.columns(3)
            with col1:
                col_product = st.selectbox(f"product name {os.path.basename(demo_files[i])}", [''] + columns, index=([''] + columns).index(product_guess) if product_guess in columns else 0, key=f"product_{i}")
            with col2:
                col_category = st.selectbox(f"category {os.path.basename(demo_files[i])}", [''] + columns, index=([''] + columns).index(category_guess) if category_guess in columns else 0, key=f"category_{i}")
            with col3:
                col_sku = st.selectbox(f"SKU column {os.path.basename(demo_files[i])}", [''] + columns, index=([''] + columns).index(sku_guess) if sku_guess in columns else 0, key=f"sku_{i}")
        # Standardize columns if selected
        rename_dict = {}
        # Always map 'Product Name' to 'product_name' for matching
        if col_product:
            if col_product.lower().strip() == 'product name':
                rename_dict[col_product] = 'product_name'
            else:
                rename_dict[col_product] = 'product_name'
        if col_category: rename_dict[col_category] = 'category'
        if col_sku: rename_dict[col_sku] = 'SKU'
        df_selected = df_demo.rename(columns=rename_dict)
        # If SKU not selected or not found, create it from product_name
        if 'SKU' not in df_selected.columns or not col_sku:
            if 'product_name' in df_selected.columns:
                df_selected['SKU'] = df_selected['product_name']
        selected_dfs.append(df_selected)
    df = pd.concat(selected_dfs, ignore_index=True)
    # Always add group_name column at start
    if 'group_name' not in df.columns:
        df['group_name'] = df['category'] if 'category' in df.columns else df.iloc[:,0]
    st.success("Demo data loaded and columns standardized.")
    st.dataframe(df)  # Show loaded demo table view
    # Reset demo flag if user uploads files
    if uploaded_files:
        st.session_state['use_demo'] = False
elif uploaded_files:
    # Add a column with the uploaded source file name after 'product_name'
    dfs = [robust_read_csv(f) for f in uploaded_files]
    # Filter out None results if a file failed to load
    dfs = [df for df in dfs if df is not None]
    if not dfs:
        st.stop() # Stop if no files could be loaded

    for i, temp_df in enumerate(dfs):
        source_name = getattr(uploaded_files[i], 'name', f'uploaded_file_{i}')
        insert_idx = temp_df.columns.get_loc('product_name') + 1 if 'product_name' in temp_df.columns else len(temp_df.columns)
        temp_df.insert(insert_idx, 'source_file', source_name)
        
        with st.expander(f"üìÅ Uploaded file: {source_name}", expanded=False):
            st.dataframe(temp_df)
            columns = list(temp_df.columns)
            def find_col(targets):
                for t in targets:
                    for c in columns:
                        if c.strip().lower() == t:
                            return c
                return ''
            product_guess = find_col(['product_name', 'product', 'name', 'product name'])
            category_guess = find_col(['category', 'cat'])
            sku_guess = find_col(['sku', '–∫–æ–¥', 'id'])
            col1, col2, col3 = st.columns(3)
            with col1:
                col_product = st.selectbox(f"product name {source_name}", [''] + columns, index=([''] + columns).index(product_guess) if product_guess in columns else 0, key=f"product_up_{source_name}")
            with col2:
                col_category = st.selectbox(f"category  {source_name}", [''] + columns, index=([''] + columns).index(category_guess) if category_guess in columns else 0, key=f"category_up_{source_name}")
            with col3:
                col_sku = st.selectbox(f"SKU column for {source_name}", [''] + columns, index=([''] + columns).index(sku_guess) if sku_guess in columns else 0, key=f"sku_up_{source_name}")
        rename_dict = {}
        # Always map 'Product Name' to 'product_name' for matching
        if col_product:
            if col_product.lower().strip() == 'product name':
                rename_dict[col_product] = 'product_name'
            else:
                rename_dict[col_product] = 'product_name'
        if col_category: rename_dict[col_category] = 'category'
        if col_sku: rename_dict[col_sku] = 'SKU'
        temp_df = temp_df.rename(columns=rename_dict)
        # If SKU not selected or not found, create it from product_name
        if 'SKU' not in temp_df.columns or not col_sku:
            if 'product_name' in temp_df.columns:
                temp_df['SKU'] = temp_df['product_name']
    df = pd.concat(dfs, ignore_index=True)
    # Always add group_name column at start
    if 'group_name' not in df.columns:
        df['group_name'] = df['category'] if 'category' in df.columns else df.iloc[:,0]
    st.success("Uploaded files loaded and columns standardized.")
    st.dataframe(df)
else:
    df = None

if df is not None:
    # –ü–∞–Ω–µ–ª—å –Ω–∞—Å—Ç—Ä–æ–µ–∫
    st.markdown("### –ü–∞–Ω–µ–ª—å –Ω–∞—Å—Ç—Ä–æ–µ–∫")
    
    col1, col2 = st.columns(2)
    
    with col1:
        group_only_diff_sources = st.checkbox(
            "–ì—Ä—É–ø–ø–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤",
            value=True,
            help="–ï—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ: –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤. –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –æ—Å—Ç–∞—é—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω—ã–º–∏. –ï—Å–ª–∏ –æ—Ç–∫–ª—é—á–µ–Ω–æ: –≥—Ä—É–ø–ø–∏—Ä—É—é—Ç—Å—è –≤—Å–µ –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–∞."
        )
        
        # –î–æ–±–∞–≤–ª—è–µ–º —á–µ–∫–±–æ–∫—Å –¥–ª—è —Ä—É—á–Ω–æ–≥–æ —Ä–µ–∂–∏–º–∞
        manual_mode = st.checkbox(
            "–†—É—á–Ω–æ–π —Ä–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏ (manual mode)",
            value=False,
            help="–í –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–º —Ä–µ–∂–∏–º–µ –≤—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (–∫—Ä–æ–º–µ ungrouped) –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏. –í —Ä—É—á–Ω–æ–º —Ä–µ–∂–∏–º–µ –≤—ã –º–æ–∂–µ—Ç–µ –≤—ã–±–∏—Ä–∞—Ç—å –∫–∞–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –æ–±—ä–µ–¥–∏–Ω—è—Ç—å."
        )
    
    with col2:
        # üå≥ –ù–û–í–´–ô –ß–ï–ö–ë–û–ö–° –î–õ–Ø –ò–ï–†–ê–†–•–ò–ß–ï–°–ö–û–ô –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–ò
        hierarchical_mode = st.checkbox(
            "üå≥ –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π",
            value=False,
            help="–í–∫–ª—é—á–∏—Ç–µ –µ—Å–ª–∏ –≤–∞—à–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–º–µ—é—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É '–†–æ–¥–∏—Ç–µ–ª—å/–ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è/–ü–æ–¥–ø–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏—è' (–Ω–∞–ø—Ä–∏–º–µ—Ä: 'Bench Tools/Circular saws'). –≠—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ —É–ª—É—á—à–∏—Ç –∫–∞—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö."
        )
        
        if hierarchical_mode:
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞
            separator = st.text_input(
                "–†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å —É—Ä–æ–≤–Ω–µ–π", 
                value="/", 
                help="–°–∏–º–≤–æ–ª —Ä–∞–∑–¥–µ–ª—è—é—â–∏–π —É—Ä–æ–≤–Ω–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–π (–æ–±—ã—á–Ω–æ '/' –∏–ª–∏ '\\')"
            )
            
            clustering_strategy = st.selectbox(
                "–°—Ç—Ä–∞—Ç–µ–≥–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏",
                ["weighted_combined", "detailed_first", "parent_fallback"],
                format_func=lambda x: {
                    "weighted_combined": "üéØ –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)",
                    "detailed_first": "üìä –°–Ω–∞—á–∞–ª–∞ –¥–µ—Ç–∞–ª—å–Ω—ã–µ ‚Üí –ø–æ—Ç–æ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ",
                    "parent_fallback": "üîÑ –ü–æ–¥–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ‚Üí –µ—Å–ª–∏ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–æ, —Ç–æ —Ä–æ–¥–∏—Ç–µ–ª–∏"
                }[x],
                help="–ö–∞–∫ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"
            )

    # 2. Select category column (only columns with 'category' in name are suggested)
    category_candidates = [col for col in df.columns if 'category' in col.lower()]
    if category_candidates:
        category_col = st.selectbox("Select the category column", category_candidates)
    else:
        category_col = st.selectbox("Select the category column", df.columns)

    # 3. Select embedding model
    model_name = st.selectbox(
        "Select embedding model",
        ["sentence-transformers/distiluse-base-multilingual-cased"]
    )
    
    if model_name == "sentence-transformers/distiluse-base-multilingual-cased":
        
        try:
            # --- Iterative clustering: use current_df for all rounds ---
            if 'current_df' not in st.session_state:
                st.session_state['current_df'] = df.copy()
            if 'manual_join_selected' not in st.session_state:
                st.session_state['manual_join_selected'] = set()
            if 'clusters_to_explode' not in st.session_state:
                st.session_state['clusters_to_explode'] = set()

            current_df = st.session_state['current_df']
            manual_join_selected = st.session_state['manual_join_selected']


            # --- Exclude fixed clusters from clustering ---
            # 1. Split current_df into fixed and unfixed parts
            fixed_group_names = st.session_state.get('fixed_group_names', set())
            if 'group_name' in current_df.columns:
                # –ë–µ—Ä–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã
                fixed_mask = current_df['group_name'].isin(fixed_group_names)
                unfixed_df = current_df[~fixed_mask].copy()
                
                # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ë–µ—Ä–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏, –Ω–µ group_name
                categories = unfixed_df[category_col].astype(str).unique()
            else:
                fixed_mask = current_df[category_col].isin(fixed_group_names)
                unfixed_df = current_df[~fixed_mask].copy()
                categories = unfixed_df[category_col].astype(str).unique()

            if len(categories) == 0:
                st.warning("No more categories left for clustering. All clusters are fixed.")
                df_clusters = pd.DataFrame({"original_category": [], "cluster": []})
            else:
                # üå≥ –ò–ï–†–ê–†–•–ò–ß–ï–°–ö–ê–Ø –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–ò–Ø
                if hierarchical_mode:
                    st.info("üå≥ –ò—Å–ø–æ–ª—å–∑—É—é –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                    
                    # –°–æ–∑–¥–∞–µ–º DataFrame —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ –∏ —É–∫–∞–∑—ã–≤–∞–µ–º –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ –∫–∞–∫ —Å—Ç—Ä–æ–∫—É
                    temp_df = pd.DataFrame({'category': categories})
                    
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∏–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é
                    clustered_df = hierarchical_clustering(
                        temp_df,  
                        'category',  # –ü–µ—Ä–µ–¥–∞–µ–º –∏–º—è –∫–æ–ª–æ–Ω–∫–∏ –∫–∞–∫ —Å—Ç—Ä–æ–∫—É
                        separator=separator,
                        strategy=clustering_strategy,
                        threshold=0.35  # –ú–æ–∂–Ω–æ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å
                    )
                    
                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    df_clusters = pd.DataFrame({
                        "original_category": clustered_df['category'].tolist(),
                        "cluster": clustered_df['cluster'].tolist()
                    })
                    
                    st.success(f"‚úÖ –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –ù–∞–π–¥–µ–Ω–æ {len(df_clusters['cluster'].unique())} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
                    
                else:
                    # –û–±—ã—á–Ω–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
                    with st.spinner("Loading model and generating embeddings. This may take a few minutes on first run..."):
                        import torch
                        # Try to force CPU and handle meta tensor error
                        try:
                            model = SentenceTransformer(model_name, device="cpu")
                            # Check for meta tensors and reload if needed
                            for name, param in model.named_parameters():
                                if hasattr(param, 'is_meta') and param.is_meta:
                                    st.warning(f"Parameter {name} is a meta tensor. Attempting to reload on CPU...")
                                    model = SentenceTransformer(model_name, device="cpu", trust_remote_code=True)
                                    break
                            embeddings = model.encode(categories, show_progress_bar=False)
                            st.write("Embeddings shape:", np.array(embeddings).shape)
                        except RuntimeError as e:
                            st.error(f"PyTorch RuntimeError: {e}\n\n–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å PyTorch –∏ sentence-transformers, –ª–∏–±–æ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ–ª—å–∫–æ CPU-–æ–∫—Ä—É–∂–µ–Ω–∏–µ.")
                            st.stop()
                        except Exception as e:
                            st.error(f"Model loading failed: {e}\n\nThis error may be caused by a mismatch between PyTorch and your hardware. Try updating PyTorch, or running on a different machine/environment.")
                            st.stop()

                    eps = st.slider("DBSCAN eps (distance threshold)", 0.1, 1.0, 0.4, 0.05, key="eps_main")
                    min_samples = st.slider("DBSCAN min_samples", 1, 5, 2, key="min_samples_main")
                    clustering = DBSCAN(eps=eps, min_samples=min_samples, metric="cosine").fit(embeddings)
                    df_clusters = pd.DataFrame({
                        "original_category": categories,
                        "cluster": clustering.labels_
                    })

            # –ê–í–¢–û–ú–ê–¢–ò–ß–ï–°–ö–ò–ô –†–ï–ñ–ò–ú - –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (–∫—Ä–æ–º–µ ungrouped) –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
            if not manual_mode and len(df_clusters) > 0:
                st.markdown("### ü§ñ –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–µ–Ω")
                st.info("–í—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã (–∫—Ä–æ–º–µ ungrouped) –æ–±—ä–µ–¥–∏–Ω—è—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
                st.info("üí° **–°–æ–≤–µ—Ç:** –í–∫–ª—é—á–∏—Ç–µ '–†—É—á–Ω–æ–π —Ä–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏' –≤—ã—à–µ, –µ—Å–ª–∏ —Ö–æ—Ç–∏—Ç–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–ª–∏ —Ä–∞–∑–±–∏—Ç—å —É–∂–µ —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã")
                
                # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ–º –≤—Å–µ –∫–ª–∞—Å—Ç–µ—Ä—ã –∫—Ä–æ–º–µ -1 (ungrouped)
                auto_renames = {}
                auto_fixed_group_names = set()
                manual_fixed_assignments = st.session_state.get('manual_fixed_assignments', {})
                
                for cluster_id in df_clusters["cluster"].unique():
                    if cluster_id != -1:  # –ò—Å–∫–ª—é—á–∞–µ–º ungrouped
                        cluster_cats = df_clusters[df_clusters["cluster"] == cluster_id]["original_category"].tolist()
                        if len(cluster_cats) > 1:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ –±–æ–ª—å—à–µ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                            group_name = cluster_cats[0]
                            for cat in cluster_cats:
                                auto_renames[cat] = group_name
                                auto_fixed_group_names.add(group_name)
                                # Track manual assignments for history
                                if group_name not in manual_fixed_assignments:
                                    manual_fixed_assignments[group_name] = set()
                                manual_fixed_assignments[group_name].add(cat)
                
                if auto_renames:
                    # Save manual assignments for all rounds
                    st.session_state['manual_fixed_assignments'] = manual_fixed_assignments
                    st.session_state['fixed_group_names'] = st.session_state.get('fixed_group_names', set()) | auto_fixed_group_names
                    
                    # Remove group_name if exists to avoid merge conflicts
                    if 'group_name' in current_df.columns:
                        current_df = current_df.drop(columns=['group_name'])
                    group_table = df_clusters.copy()
                    group_table["group_name"] = group_table["original_category"].map(lambda x: auto_renames.get(x, x))
                    new_df = current_df.merge(group_table[["original_category", "group_name"]], left_on=(category_col), right_on="original_category", how="left")
                    
                    # Filter by source_file if needed
                    if group_only_diff_sources and "source_file" in new_df.columns and "group_name" in new_df.columns:
                        new_df['source_file_count'] = new_df.groupby('group_name')['source_file'].transform('nunique')
                        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –º–∞—Å–∫–∞ –¥–ª—è ungrouped
                        mask_ungrouped = new_df['group_name'] == new_df[category_col]  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                        mask_grouped = new_df['source_file_count'] > 1  # –ì—Ä—É–ø–ø—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                        new_df = new_df[mask_ungrouped | mask_grouped].drop(columns=['source_file_count'])
                        
                        # Debug –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏
                        st.write(f"üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º:")
                        st.write(f"- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(new_df[new_df['group_name'] == new_df[category_col]])}")
                        st.write(f"- –°–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(new_df[new_df['group_name'] != new_df[category_col]])}")
                        
                    if "original_category" in new_df.columns:
                        new_df = new_df.drop(columns=["original_category"])
                    st.session_state['current_df'] = new_df
                    
                    st.success(f"–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–æ {len(auto_fixed_group_names)} –≥—Ä—É–ø–ø –∫–∞—Ç–µ–≥–æ—Ä–∏–π")
                    # Show only key columns in intermediate table
                    show_cols = [col for col in ['group_name', 'category', 'product_name', 'SKU', 'source_file'] if col in new_df.columns]
                    st.dataframe(new_df[show_cols])
                else:
                    st.info("–ù–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è")

            # –†–£–ß–ù–û–ô –†–ï–ñ–ò–ú - –ø–æ–∫–∞–∑—ã–≤–∞–µ–º –∫–ª–∞—Å—Ç–µ—Ä—ã –¥–ª—è –≤—ã–±–æ—Ä–∞
            elif manual_mode:
                st.markdown("### üîß –†—É—á–Ω–æ–π —Ä–µ–∂–∏–º –∞–∫—Ç–∏–≤–µ–Ω")
                st.markdown("#### Review clusters and select those you want to join (rename all to first):")
                
                # Show all fixed clusters from all rounds (not just current fixed_df)
                fixed_group_names_all = st.session_state.get('fixed_group_names', set())
                if fixed_group_names_all:
                    st.markdown("---")
                    st.markdown("#### Fixed clusters (excluded from further clustering):")
                    # Find all categories assigned to each fixed group_name in ALL previous rounds
                    manual_fixed_assignments = st.session_state.get('manual_fixed_assignments', {})
                    
                    # Track which clusters to explode
                    if 'clusters_to_explode' not in st.session_state:
                        st.session_state['clusters_to_explode'] = set()
                    clusters_to_explode = st.session_state['clusters_to_explode']
                    
                    for fixed_val in sorted(fixed_group_names_all):
                        assigned_cats = set()
                        # 1. From manual_fixed_assignments (if exists)
                        if fixed_val in manual_fixed_assignments:
                            assigned_cats.update(manual_fixed_assignments[fixed_val])
                        # 2. From all current and previous DataFrames
                        for search_df in [st.session_state.get('current_df', df), df]:
                            if 'group_name' in search_df.columns:
                                assigned_cats.update(search_df[search_df['group_name'] == fixed_val][category_col].unique())
                        # Fallback: if nothing found, just show the group name itself
                        if not assigned_cats:
                            assigned_cats = {fixed_val}
                        
                        with st.expander(f"Fixed cluster: {fixed_val} ({len(assigned_cats)} –∫–∞—Ç–µ–≥–æ—Ä–∏–π)"):
                            st.success(list(assigned_cats))
                            
                            # –î–æ–±–∞–≤–ª—è–µ–º —á–µ–∫–±–æ–∫—Å –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞
                            explode_checked = st.checkbox(
                                f"üîÑ Explode this cluster (—Ä–∞–∑–±–∏—Ç—å –≥—Ä—É–ø–ø—É '{fixed_val}' –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏)",
                                key=f"explode_checkbox_{fixed_val}",
                                value=(fixed_val in clusters_to_explode),
                                help="–≠—Ç–æ —Ä–∞–∑–±–∏–≤–∞–µ—Ç –≥—Ä—É–ø–ø—É –æ–±—Ä–∞—Ç–Ω–æ –Ω–∞ –æ—Ç–¥–µ–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"
                            )
                            
                            if explode_checked:
                                clusters_to_explode.add(fixed_val)
                            else:
                                clusters_to_explode.discard(fixed_val)
                    
                    # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Ä–∞–∑–±–∏–µ–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
                    if clusters_to_explode:
                        st.markdown("---")
                        if st.button(f"üîÑ Explode selected clusters ({len(clusters_to_explode)} –≤—ã–±—Ä–∞–Ω–æ)", key="explode_clusters_btn"):
                            # –†–∞–∑–±–∏–≤–∞–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
                            manual_fixed_assignments = st.session_state.get('manual_fixed_assignments', {})
                            fixed_group_names = st.session_state.get('fixed_group_names', set())
                            current_df = st.session_state.get('current_df', df)
                            
                            for cluster_to_explode in clusters_to_explode:
                                # –£–¥–∞–ª—è–µ–º –∏–∑ fixed_group_names
                                fixed_group_names.discard(cluster_to_explode)
                                
                                # –£–¥–∞–ª—è–µ–º –∏–∑ manual_fixed_assignments
                                if cluster_to_explode in manual_fixed_assignments:
                                    del manual_fixed_assignments[cluster_to_explode]
                                
                                # –í current_df –≤–æ–∑–≤—Ä–∞—â–∞–µ–º group_name –æ–±—Ä–∞—Ç–Ω–æ –∫ –∏—Å—Ö–æ–¥–Ω—ã–º –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
                                if 'group_name' in current_df.columns:
                                    mask = current_df['group_name'] == cluster_to_explode
                                    current_df.loc[mask, 'group_name'] = current_df.loc[mask, category_col]
                            
                            # –û–±–Ω–æ–≤–ª—è–µ–º session_state
                            st.session_state['fixed_group_names'] = fixed_group_names
                            st.session_state['manual_fixed_assignments'] = manual_fixed_assignments
                            st.session_state['current_df'] = current_df
                            st.session_state['clusters_to_explode'] = set()  # –û—á–∏—â–∞–µ–º —Å–ø–∏—Å–æ–∫
                            
                            st.success(f"–†–∞–∑–±–∏—Ç–æ {len(clusters_to_explode)} –∫–ª–∞—Å—Ç–µ—Ä–æ–≤. –ö–∞—Ç–µ–≥–æ—Ä–∏–∏ –≥–æ—Ç–æ–≤—ã –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω–æ–π –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.")
                            st.rerun()

                # Now show clusters for manual join (only for unfixed)
                for cluster_id in sorted(df_clusters["cluster"].unique()):
                    cluster_cats = df_clusters[df_clusters["cluster"] == cluster_id]["original_category"].tolist()
                    is_ungrouped = (cluster_id == -1)
                    expander_label = f"Cluster {'ungrouped' if is_ungrouped else cluster_id}"
                    with st.expander(expander_label, expanded=True):
                        if is_ungrouped:
                            st.warning("This cluster contains ungrouped categories. They will not be merged with others until you select and apply a join.")
                        st.json(cluster_cats)
                        checked = st.checkbox(f"Join this cluster (rename all to '{cluster_cats[0]}')", key=f"join_checkbox_{cluster_id}", value=(cluster_id in manual_join_selected))
                        if checked:
                            manual_join_selected.add(cluster_id)
                        else:
                            manual_join_selected.discard(cluster_id)

                colA, colB = st.columns([1,1])
                apply_btn = colA.button("Apply Manual Joins", key="apply_manual_joins_batch")
                reset_btn = colB.button("üîÑ Reset All Groups & Re-cluster", key="reset_all_groups", 
                                      help="–°–±—Ä–æ—Å–∏—Ç—å –≤—Å–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –∑–∞–Ω–æ–≤–æ")

                if reset_btn:
                    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—Å–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                    st.session_state['fixed_group_names'] = set()
                    st.session_state['manual_fixed_assignments'] = {}
                    st.session_state['current_df'] = df.copy()
                    st.session_state['manual_join_selected'] = set()
                    st.session_state['clusters_to_explode'] = set()
                    
                    st.success("–í—Å–µ –≥—Ä—É–ø–ø—ã —Å–±—Ä–æ—à–µ–Ω—ã! –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å —Ç–µ–∫—É—â–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
                    st.rerun()

                if apply_btn:
                    renames = {}
                    fixed_group_names = set()
                    manual_fixed_assignments = st.session_state.get('manual_fixed_assignments', {})
                    for cluster_id in manual_join_selected:
                        cluster_cats = df_clusters[df_clusters["cluster"] == cluster_id]["original_category"].tolist()
                        group_name = cluster_cats[0]
                        for cat in cluster_cats:
                            renames[cat] = group_name
                            fixed_group_names.add(group_name)
                            # Track manual assignments for history
                            if group_name not in manual_fixed_assignments:
                                manual_fixed_assignments[group_name] = set()
                            manual_fixed_assignments[group_name].add(cat)
                    # Save manual assignments for all rounds
                    st.session_state['manual_fixed_assignments'] = manual_fixed_assignments
                    st.session_state['fixed_group_names'] = st.session_state.get('fixed_group_names', set()) | fixed_group_names
                    # Remove group_name if exists to avoid merge conflicts
                    if 'group_name' in current_df.columns:
                        current_df = current_df.drop(columns=['group_name'])
                    group_table = df_clusters.copy()
                    group_table["group_name"] = group_table["original_category"].map(lambda x: renames.get(x, x))
                    new_df = current_df.merge(group_table[["original_category", "group_name"]], left_on=(category_col), right_on="original_category", how="left")
                    # Only filter by source_file if group_name was actually created
                    if group_only_diff_sources and "source_file" in new_df.columns and "group_name" in new_df.columns:
                        new_df['source_file_count'] = new_df.groupby('group_name')['source_file'].transform('nunique')
                        # ‚úÖ –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è –º–∞—Å–∫–∞ –¥–ª—è ungrouped
                        mask_ungrouped = new_df['group_name'] == new_df[category_col]  # –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                        mask_grouped = new_df['source_file_count'] > 1  # –ì—Ä—É–ø–ø—ã –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                        new_df = new_df[mask_ungrouped | mask_grouped].drop(columns=['source_file_count'])
                        
                        # Debug –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏  
                        st.write(f"üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º –≤ —Ä—É—á–Ω–æ–º —Ä–µ–∂–∏–º–µ:")
                        st.write(f"- –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π: {len(new_df[new_df['group_name'] == new_df[category_col]])}")
                        st.write(f"- –°–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤: {len(new_df[new_df['group_name'] != new_df[category_col]])}")
                    if "original_category" in new_df.columns:
                        new_df = new_df.drop(columns=["original_category"])
                    st.session_state['current_df'] = new_df
                    st.session_state['manual_join_selected'] = set()
                    st.success("Manual joins applied. You can now re-run clustering on the updated table.")
                    # Show only key columns in intermediate table
                    show_cols = [col for col in ['group_name', 'category', 'product_name', 'SKU', 'source_file'] if col in new_df.columns]
                    st.dataframe(new_df[show_cols])

                # --- Manual merge block: –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –≤ —Ä—É—á–Ω–æ–º —Ä–µ–∂–∏–º–µ ---
                st.markdown("---")
                st.markdown("#### Manual merge: –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –ª—é–±—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –≤—Ä—É—á–Ω—É—é")
                # 1. –ò—Å–∫–ª—é—á–∏—Ç—å —É–∂–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏–∑ —Å–ø–∏—Å–∫–∞ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è
                fixed_group_names_set = st.session_state.get('fixed_group_names', set())
                manual_fixed_assignments = st.session_state.get('manual_fixed_assignments', {})
                # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
                fixed_cats = set()
                for group, cats in manual_fixed_assignments.items():
                    fixed_cats.update(cats)
                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –¥–ª—è –≤—ã–±–æ—Ä–∞: –µ—Å–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏—è –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–∞, –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Å –ø–æ–º–µ—Ç–∫–æ–π
                all_categories = sorted(df[category_col].astype(str).unique())
                manual_merge_options = []
                manual_merge_labels = {}
                for cat in all_categories:
                    fixed_group = None
                    for group, cats in manual_fixed_assignments.items():
                        if cat in cats:
                            fixed_group = group
                            break
                    if fixed_group:
                        manual_merge_labels[cat] = f"{cat} (—É–∂–µ –≤ –≥—Ä—É–ø–ø–µ '{fixed_group}')"
                    else:
                        manual_merge_options.append(cat)
                        manual_merge_labels[cat] = cat
                manual_merge_cats = st.multiselect(
                    "–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –¥–ª—è –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –≤—Ä—É—á–Ω—É—é",
                    options=manual_merge_options,
                    format_func=lambda x: manual_merge_labels[x],
                    key="manual_merge_cats"
                )
                manual_merge_name = st.text_input("–ù–æ–≤–æ–µ –∏–º—è –≥—Ä—É–ø–ø—ã –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π", key="manual_merge_name")
                if st.button("–ü—Ä–∏–º–µ–Ω–∏—Ç—å —Ä—É—á–Ω–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ"):
                    if manual_merge_cats and manual_merge_name:
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞: –Ω–µ –¥–∞—Ç—å –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –ø–æ–ø–∞—Å—Ç—å –≤ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≥—Ä—É–ø–ø
                        overlap = set(manual_merge_cats) & fixed_cats
                        if overlap:
                            st.error(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ {list(overlap)} —É–∂–µ –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω—ã –≤ –¥—Ä—É–≥–∏—Ö –≥—Ä—É–ø–ø–∞—Ö. –°–Ω–∏–º–∏—Ç–µ –∏—Ö –∏–∑ –≤—ã–±–æ—Ä–∞.")
                        else:
                            # –û–±–Ω–æ–≤–∏—Ç—å group_name –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤ current_df
                            current_df = st.session_state['current_df']
                            current_df.loc[current_df[category_col].isin(manual_merge_cats), 'group_name'] = manual_merge_name
                            st.session_state['current_df'] = current_df
                            # –î–æ–±–∞–≤–∏—Ç—å –≤ –∏—Å—Ç–æ—Ä–∏—é —Ä—É—á–Ω—ã—Ö –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–π
                            if manual_merge_name not in manual_fixed_assignments:
                                manual_fixed_assignments[manual_merge_name] = set()
                            manual_fixed_assignments[manual_merge_name].update(manual_merge_cats)
                            st.session_state['manual_fixed_assignments'] = manual_fixed_assignments
                            # –î–æ–±–∞–≤–∏—Ç—å –≤ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã
                            fixed_group_names = st.session_state.get('fixed_group_names', set())
                            fixed_group_names.add(manual_merge_name)
                            st.session_state['fixed_group_names'] = fixed_group_names
                            st.success(f"–ö–∞—Ç–µ–≥–æ—Ä–∏–∏ {manual_merge_cats} –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã –≤ –≥—Ä—É–ø–ø—É '{manual_merge_name}'")
                    else:
                        st.warning("–í—ã–±–µ—Ä–∏—Ç–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –∏ –≤–≤–µ–¥–∏—Ç–µ –∏–º—è –≥—Ä—É–ø–ø—ã.")

            # –ö–Ω–æ–ø–∫–∞ —Å–±—Ä–æ—Å–∞ –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–µ–∂–∏–º–∞
            if not manual_mode:
                if st.button("üîÑ Reset All Groups & Re-cluster", key="reset_all_groups_auto",
                           help="–°–±—Ä–æ—Å–∏—Ç—å –≤—Å–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –∑–∞–Ω–æ–≤–æ"):
                    # –°–±—Ä–∞—Å—ã–≤–∞–µ–º –≤—Å–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≥—Ä—É–ø–ø—ã –∏ —Å–æ—Å—Ç–æ—è–Ω–∏–µ
                    st.session_state['fixed_group_names'] = set()
                    st.session_state['manual_fixed_assignments'] = {}
                    st.session_state['current_df'] = df.copy()
                    st.session_state['manual_join_selected'] = set()
                    st.session_state['clusters_to_explode'] = set()
                    
                    st.success("–í—Å–µ –≥—Ä—É–ø–ø—ã —Å–±—Ä–æ—à–µ–Ω—ã! –ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å —Ç–µ–∫—É—â–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏...")
                    st.rerun()
            
            # 8. Download final processed file (from current_df)
            st.markdown("---")
            st.markdown("#### Download the final processed table (all manual and automatic merges applied):")
            # Build a full final table: merge all fixed and unfixed rows from the original df
            # 1. Get all fixed group names and their members from session_state
            fixed_group_names = st.session_state.get('fixed_group_names', set())
            current_df = st.session_state.get('current_df', df)
            # Build fixed assignments from ALL rounds, not just current_df
            fixed_assignments = {}
            manual_fixed_assignments = st.session_state.get('manual_fixed_assignments', {})
            for fixed_group in fixed_group_names:
                # 1. From manual_fixed_assignments (if exists)
                if fixed_group in manual_fixed_assignments:
                    for cat in manual_fixed_assignments[fixed_group]:
                        fixed_assignments[cat] = fixed_group
                # 2. From all current and previous DataFrames
                for search_df in [st.session_state.get('current_df', df), df]:
                    if 'group_name' in search_df.columns:
                        matches = search_df[search_df['group_name'] == fixed_group]
                        for _, row in matches.iterrows():
                            fixed_assignments[row[category_col]] = fixed_group
            # 2. For all rows in the original df, assign group_name: fixed if exists, else from current_df, else itself
            def get_final_group(row):
                cat = row[category_col]
                # Priority: fixed assignment > current_df group_name > ungrouped marker
                if cat in fixed_assignments:
                    return fixed_assignments[cat]
                # Try to get from current_df (may be unfixed group)
                if 'group_name' in current_df.columns:
                    match = current_df[current_df[category_col] == cat]
                    if not match.empty:
                        val = match.iloc[0]['group_name']
                        # If group_name is not the same as the original category, treat as grouped
                        if val != cat:
                            return val
                # If not grouped, mark as ungrouped (use -1, or 'ungrouped')
                return 'ungrouped'
            final_df = df.copy()
            final_df['group_name'] = final_df.apply(get_final_group, axis=1)
            # Remove only service columns if present
            for col in ["original_category", "group_name_x", "group_name_y"]:
                if col in final_df.columns:
                    final_df = final_df.drop(columns=[col])
            st.dataframe(final_df.head(20))
            st.download_button(
                "Download final grouped table CSV",
                final_df.to_csv(index=False, encoding="utf-8-sig").encode("utf-8-sig"),
                "final_grouped_table.csv",
                "text/csv"
            )
        except NotImplementedError as e:
            st.error(f"Model loading failed: {e}\n\n"
                     "This error may be caused by a mismatch between PyTorch and your hardware. "
                     "Try updating PyTorch, or running on a different machine/environment.")
        
        # –î–æ–±–∞–≤–∏—Ç—å –ø–æ—Å–ª–µ —Å–µ–∫—Ü–∏–∏ "Download the final processed table":
        st.markdown("---")
        st.markdown("#### üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏:")
        col1, col2, col3 = st.columns(3)
        with col1:
            total_categories = len(df[category_col].unique())
            st.metric("–í—Å–µ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π", total_categories)
        with col2:
            grouped_categories = len([g for g in final_df['group_name'].unique() if g != 'ungrouped'])
            st.metric("–°–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö", grouped_categories)
        with col3:
            reduction_percent = round((1 - grouped_categories/total_categories) * 100, 1) if total_categories > 0 else 0
            st.metric("–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ", f"{reduction_percent}%")

        # –¢–æ–ø –≥—Ä—É–ø–ø –ø–æ —Ä–∞–∑–º–µ—Ä—É
        st.markdown("**–¢–æ–ø-5 —Å–∞–º—ã—Ö –±–æ–ª—å—à–∏—Ö –≥—Ä—É–ø–ø:**")
        group_sizes = final_df[final_df['group_name'] != 'ungrouped']['group_name'].value_counts().head(5)
        st.bar_chart(group_sizes)
else:
    st.info("Please upload at least one CSV file.")

# --- –ö–Ω–æ–ø–∫–∏ –ø–µ—Ä–µ—Ö–æ–¥–∞ –Ω–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã ---

if df is not None:
    st.markdown("---")
    st.markdown("### –î–∞–ª—å–Ω–µ–π—à–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö")
    
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("üìè –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤", key="go_to_standardization_page"):
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–µ –¥–∞–Ω–Ω—ã–µ –≤ grouped_categories.csv
            fixed_group_names = st.session_state.get('fixed_group_names', set())
            current_df = st.session_state.get('current_df', df)
            
            # Build fixed assignments from ALL rounds
            fixed_assignments = {}
            manual_fixed_assignments = st.session_state.get('manual_fixed_assignments', {})
            for fixed_group in fixed_group_names:
                # 1. From manual_fixed_assignments (if exists)
                if fixed_group in manual_fixed_assignments:
                    for cat in manual_fixed_assignments[fixed_group]:
                        fixed_assignments[cat] = fixed_group
                # 2. From all current and previous DataFrames
                for search_df in [st.session_state.get('current_df', df), df]:
                    if 'group_name' in search_df.columns:
                        matching_rows = search_df[search_df['group_name'] == fixed_group]
                        for cat in matching_rows['category']:
                            fixed_assignments[cat] = fixed_group
            
            # Apply fixed assignments to the current DataFrame
            final_table = current_df.copy()
            if fixed_assignments:
                for cat, group in fixed_assignments.items():
                    final_table.loc[final_table['category'] == cat, 'group_name'] = group
            
            # Save to grouped_categories.csv
            save_path = os.path.join(os.getcwd(), "grouped_categories.csv")
            try:
                final_table.to_csv(save_path, index=False, encoding="utf-8-sig")
                st.info(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –¥–ª—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏–∏ –≤: {save_path}")
            except Exception as e:
                st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å grouped_categories.csv: {e}")
            st.switch_page('pages/standardization.py')
    
    with col2:
        if st.button("‚öôÔ∏è –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤", key="go_to_param_page"):
            # –°—Ç—Ä–æ–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Ç–æ—á–Ω–æ —Ç–∞–∫ –∂–µ, –∫–∞–∫ –≤ —Å–µ–∫—Ü–∏–∏ "Download the final processed table"
            fixed_group_names = st.session_state.get('fixed_group_names', set())
            current_df = st.session_state.get('current_df', df)
            
            # Build fixed assignments from ALL rounds
            fixed_assignments = {}
            manual_fixed_assignments = st.session_state.get('manual_fixed_assignments', {})
            for fixed_group in fixed_group_names:
                # 1. From manual_fixed_assignments (if exists)
                if fixed_group in manual_fixed_assignments:
                    for cat in manual_fixed_assignments[fixed_group]:
                        fixed_assignments[cat] = fixed_group
                # 2. From all current and previous DataFrames
                for search_df in [st.session_state.get('current_df', df), df]:
                    if 'group_name' in search_df.columns:
                        matches = search_df[search_df['group_name'] == fixed_group]
                        for _, row in matches.iterrows():
                            fixed_assignments[row[category_col]] = fixed_group
            
            # For all rows in the original df, assign group_name: fixed if exists, else from current_df, else itself
            def get_final_group(row):
                cat = row[category_col]
                # Priority: fixed assignment > current_df group_name > ungrouped marker
                if cat in fixed_assignments:
                    return fixed_assignments[cat]
                # Try to get from current_df (may be unfixed group)
                if 'group_name' in current_df.columns:
                    match = current_df[current_df[category_col] == cat]
                    if not match.empty:
                        val = match.iloc[0]['group_name']
                        # If group_name is not the same as the original category, treat as grouped
                        if val != cat:
                            return val
                # If not grouped, mark as ungrouped (use -1, or 'ungrouped')
                return 'ungrouped'
                
            final_table = df.copy()
            final_table['group_name'] = final_table.apply(get_final_group, axis=1)
            # Remove only service columns if present
            for col in ["original_category", "group_name_x", "group_name_y"]:
                if col in final_table.columns:
                    final_table = final_table.drop(columns=[col])
                
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –≤ grouped_categories.csv (–¥–ª—è –ø–µ—Ä–µ—Ö–æ–¥–∞ –∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏)
            save_path = os.path.join(os.getcwd(), "grouped_categories.csv")
            try:
                final_table.to_csv(save_path, index=False, encoding="utf-8-sig")
                st.info(f"–§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {save_path}")
            except Exception as e:
                st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å grouped_categories.csv: {e}")
            st.switch_page('pages/param_processing.py')

st.markdown("**Instructions:** Upload CSVs, select the category column, choose an embedding model, adjust clustering, and download the mapping.")
